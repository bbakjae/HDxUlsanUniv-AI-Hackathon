"""
ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
Gradio ê¸°ë°˜ ì±—ë´‡ UI + ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©
"""

import os
# Gradio ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì„¤ì • (ì™¸ë¶€ CDN ì˜ì¡´ì„± ì œê±°)
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"

import sys
from pathlib import Path
import yaml
import logging
from typing import List, Dict, Tuple, Optional
import gradio as gr
import numpy as np
import hashlib
import re
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.parsers.multimodal_parser import MultimodalParser
from src.embeddings.embedding_model import BGEM3Embedder
from src.search.vector_store import QdrantVectorStore
from src.search.bm25_search import BM25SearchEngine
from src.search.hybrid_search import HybridSearchEngine
from src.llm.qwen_model import QwenSummarizer, CachedSummarizer, LLMConfig
from src.recommend.recommender import FileRecommender

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class QueryParser:
    """
    ìì—°ì–´ ì¿¼ë¦¬ íŒŒì‹± - ì‹œê°„ í‘œí˜„, í•„í„° ì¡°ê±´ ì¶”ì¶œ
    """

    # ì‹œê°„ í‘œí˜„ íŒ¨í„´
    TIME_PATTERNS = {
        # ìƒëŒ€ ì‹œê°„ í‘œí˜„
        r'ì‘ë…„': ('year', -1),
        r'ì˜¬í•´': ('year', 0),
        r'ì¬ì‘ë…„': ('year', -2),
        r'ë‚´ë…„': ('year', 1),
        r'ì§€ë‚œ\s*ë‹¬': ('month', -1),
        r'ì´ë²ˆ\s*ë‹¬': ('month', 0),
        r'ì €ë²ˆ\s*ë‹¬': ('month', -1),
        r'ë‹¤ìŒ\s*ë‹¬': ('month', 1),
        r'ì§€ë‚œ\s*ì£¼': ('week', -1),
        r'ì´ë²ˆ\s*ì£¼': ('week', 0),
        r'ì €ë²ˆ\s*ì£¼': ('week', -1),
        r'ë‹¤ìŒ\s*ì£¼': ('week', 1),
        r'ì–´ì œ': ('day', -1),
        r'ì˜¤ëŠ˜': ('day', 0),
        r'ê·¸ì œ': ('day', -2),
        r'ë‚´ì¼': ('day', 1),
        r'ìµœê·¼\s*(\d+)\s*ì¼': ('recent_days', None),
        r'ìµœê·¼\s*(\d+)\s*ì£¼': ('recent_weeks', None),
        r'ìµœê·¼\s*(\d+)\s*ê°œì›”': ('recent_months', None),
        r'(\d+)\s*ì¼\s*ì „': ('days_ago', None),
        r'(\d+)\s*ì£¼\s*ì „': ('weeks_ago', None),
        r'(\d+)\s*ê°œì›”\s*ì „': ('months_ago', None),
        # ì ˆëŒ€ ì—°ë„ (2020~2030)
        r'(20[2-3]\d)ë…„': ('absolute_year', None),
        # ìƒë°˜ê¸°/í•˜ë°˜ê¸°
        r'(20[2-3]\d)ë…„?\s*ìƒë°˜ê¸°': ('half_year_1', None),
        r'(20[2-3]\d)ë…„?\s*í•˜ë°˜ê¸°': ('half_year_2', None),
        r'ìƒë°˜ê¸°': ('current_half_1', None),
        r'í•˜ë°˜ê¸°': ('current_half_2', None),
        # ë¶„ê¸°
        r'(\d)ë¶„ê¸°': ('quarter', None),
    }

    # íŒŒì¼ íƒ€ì… íŒ¨í„´
    FILE_TYPE_PATTERNS = {
        r'pdf\s*(íŒŒì¼|ë¬¸ì„œ)?': 'pdf',
        r'ì›Œë“œ\s*(íŒŒì¼|ë¬¸ì„œ)?': 'docx',
        r'docx?\s*(íŒŒì¼|ë¬¸ì„œ)?': 'docx',
        r'ì—‘ì…€\s*(íŒŒì¼|ë¬¸ì„œ)?': 'xlsx',
        r'xlsx?\s*(íŒŒì¼|ë¬¸ì„œ)?': 'xlsx',
        r'íŒŒì›Œí¬ì¸íŠ¸\s*(íŒŒì¼|ë¬¸ì„œ)?': 'pptx',
        r'pptx?\s*(íŒŒì¼|ë¬¸ì„œ)?': 'pptx',
        r'ppt\s*(íŒŒì¼|ë¬¸ì„œ)?': 'pptx',
        r'ì´ë¯¸ì§€\s*(íŒŒì¼)?': 'image',
        r'ì‚¬ì§„\s*(íŒŒì¼)?': 'image',
        r'(png|jpg|jpeg)\s*(íŒŒì¼)?': 'image',
    }

    # ë¶€ì„œ íŒ¨í„´
    DEPARTMENT_PATTERNS = {
        r'ê¸°íšíŒ€': 'ê¸°íšíŒ€',
        r'ê°œë°œíŒ€': 'ê°œë°œíŒ€',
        r'ë§ˆì¼€íŒ…íŒ€': 'ë§ˆì¼€íŒ…íŒ€',
        r'ì˜ì—…íŒ€': 'ì˜ì—…íŒ€',
        r'ì¸ì‚¬íŒ€': 'ì¸ì‚¬íŒ€',
        r'ì¬ë¬´íŒ€': 'ì¬ë¬´íŒ€',
        r'ë””ìì¸íŒ€': 'ë””ìì¸íŒ€',
        r'í’ˆì§ˆê´€ë¦¬íŒ€': 'í’ˆì§ˆê´€ë¦¬íŒ€',
        r'í’ˆì§ˆíŒ€': 'í’ˆì§ˆê´€ë¦¬íŒ€',
    }

    def __init__(self):
        self.now = datetime.now()

    def parse(self, query: str) -> Dict:
        """
        ì¿¼ë¦¬ë¥¼ íŒŒì‹±í•˜ì—¬ í•„í„° ì¡°ê±´ê³¼ ì •ì œëœ ì¿¼ë¦¬ ë°˜í™˜

        Args:
            query: ì›ë³¸ ì¿¼ë¦¬

        Returns:
            {
                'cleaned_query': str,  # í•„í„° í‘œí˜„ ì œê±°ëœ ì¿¼ë¦¬
                'date_filter': {       # ë‚ ì§œ í•„í„°
                    'start_date': datetime,
                    'end_date': datetime
                },
                'file_type': str,      # íŒŒì¼ íƒ€ì… í•„í„°
                'department': str      # ë¶€ì„œ í•„í„°
            }
        """
        result = {
            'cleaned_query': query,
            'date_filter': None,
            'file_type': None,
            'department': None
        }

        cleaned_query = query

        # 1. ì‹œê°„ í‘œí˜„ íŒŒì‹±
        date_filter = self._parse_time_expression(query)
        if date_filter:
            result['date_filter'] = date_filter
            # ì‹œê°„ í‘œí˜„ ì œê±°
            for pattern in self.TIME_PATTERNS.keys():
                cleaned_query = re.sub(pattern, '', cleaned_query, flags=re.IGNORECASE)

        # 2. íŒŒì¼ íƒ€ì… íŒŒì‹±
        for pattern, file_type in self.FILE_TYPE_PATTERNS.items():
            if re.search(pattern, query, re.IGNORECASE):
                result['file_type'] = file_type
                cleaned_query = re.sub(pattern, '', cleaned_query, flags=re.IGNORECASE)
                break

        # 3. ë¶€ì„œ íŒŒì‹±
        for pattern, department in self.DEPARTMENT_PATTERNS.items():
            if re.search(pattern, query, re.IGNORECASE):
                result['department'] = department
                # ë¶€ì„œëª…ì€ ê²€ìƒ‰ì— ìœ ìš©í•˜ë¯€ë¡œ ì œê±°í•˜ì§€ ì•ŠìŒ
                break

        # ì •ì œëœ ì¿¼ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°)
        result['cleaned_query'] = ' '.join(cleaned_query.split()).strip()

        return result

    def _parse_time_expression(self, query: str) -> Optional[Dict]:
        """ì‹œê°„ í‘œí˜„ì„ íŒŒì‹±í•˜ì—¬ ë‚ ì§œ ë²”ìœ„ ë°˜í™˜"""
        for pattern, (time_type, offset) in self.TIME_PATTERNS.items():
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                return self._calculate_date_range(time_type, offset, match)
        return None

    def _calculate_date_range(self, time_type: str, offset: Optional[int], match) -> Dict:
        """ì‹œê°„ íƒ€ì…ì— ë”°ë¥¸ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
        now = self.now

        if time_type == 'year':
            year = now.year + offset
            return {
                'start_date': datetime(year, 1, 1),
                'end_date': datetime(year, 12, 31, 23, 59, 59)
            }

        elif time_type == 'month':
            target = now + relativedelta(months=offset)
            start = datetime(target.year, target.month, 1)
            end = start + relativedelta(months=1) - timedelta(seconds=1)
            return {'start_date': start, 'end_date': end}

        elif time_type == 'week':
            # ì£¼ì˜ ì‹œì‘ì„ ì›”ìš”ì¼ë¡œ ê°€ì •
            days_since_monday = now.weekday()
            week_start = now - timedelta(days=days_since_monday) + timedelta(weeks=offset)
            week_start = datetime(week_start.year, week_start.month, week_start.day)
            week_end = week_start + timedelta(days=6, hours=23, minutes=59, seconds=59)
            return {'start_date': week_start, 'end_date': week_end}

        elif time_type == 'day':
            target = now + timedelta(days=offset)
            start = datetime(target.year, target.month, target.day)
            end = start + timedelta(hours=23, minutes=59, seconds=59)
            return {'start_date': start, 'end_date': end}

        elif time_type == 'recent_days':
            days = int(match.group(1))
            return {
                'start_date': now - timedelta(days=days),
                'end_date': now
            }

        elif time_type == 'recent_weeks':
            weeks = int(match.group(1))
            return {
                'start_date': now - timedelta(weeks=weeks),
                'end_date': now
            }

        elif time_type == 'recent_months':
            months = int(match.group(1))
            return {
                'start_date': now - relativedelta(months=months),
                'end_date': now
            }

        elif time_type == 'days_ago':
            days = int(match.group(1))
            target = now - timedelta(days=days)
            return {
                'start_date': datetime(target.year, target.month, target.day),
                'end_date': datetime(target.year, target.month, target.day, 23, 59, 59)
            }

        elif time_type == 'weeks_ago':
            weeks = int(match.group(1))
            target = now - timedelta(weeks=weeks)
            start = target - timedelta(days=target.weekday())
            return {
                'start_date': datetime(start.year, start.month, start.day),
                'end_date': datetime(start.year, start.month, start.day) + timedelta(days=6, hours=23, minutes=59, seconds=59)
            }

        elif time_type == 'months_ago':
            months = int(match.group(1))
            target = now - relativedelta(months=months)
            start = datetime(target.year, target.month, 1)
            end = start + relativedelta(months=1) - timedelta(seconds=1)
            return {'start_date': start, 'end_date': end}

        elif time_type == 'absolute_year':
            year = int(match.group(1))
            return {
                'start_date': datetime(year, 1, 1),
                'end_date': datetime(year, 12, 31, 23, 59, 59)
            }

        elif time_type == 'half_year_1':
            year = int(match.group(1))
            return {
                'start_date': datetime(year, 1, 1),
                'end_date': datetime(year, 6, 30, 23, 59, 59)
            }

        elif time_type == 'half_year_2':
            year = int(match.group(1))
            return {
                'start_date': datetime(year, 7, 1),
                'end_date': datetime(year, 12, 31, 23, 59, 59)
            }

        elif time_type == 'current_half_1':
            return {
                'start_date': datetime(now.year, 1, 1),
                'end_date': datetime(now.year, 6, 30, 23, 59, 59)
            }

        elif time_type == 'current_half_2':
            return {
                'start_date': datetime(now.year, 7, 1),
                'end_date': datetime(now.year, 12, 31, 23, 59, 59)
            }

        elif time_type == 'quarter':
            quarter = int(match.group(1))
            start_month = (quarter - 1) * 3 + 1
            return {
                'start_date': datetime(now.year, start_month, 1),
                'end_date': datetime(now.year, start_month, 1) + relativedelta(months=3) - timedelta(seconds=1)
            }

        return None


class AIAgentPipeline:
    """ì „ì²´ AI Agent íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config_path: str):
        """
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # ì„¤ì • ë¡œë“œ
        logger.info(f"Loading config from {config_path}")
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()

    def _initialize_components(self):
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("Initializing AI Agent components...")

        self.llm_enabled = bool(self.config.get('llm', {}).get('enabled', False))

        # 1. ì„ë² ë”© ëª¨ë¸
        logger.info("Loading embedding model...")
        self.embedder = BGEM3Embedder(
            model_name=self.config['embedding']['model_name'],
            device=self.config['embedding']['device'],
            use_fp16=self.config['embedding']['use_fp16']
        )

        # 2. ë²¡í„° ìŠ¤í† ì–´
        logger.info("Connecting to vector store...")
        self.vector_store = QdrantVectorStore(
            storage_path=self.config['data']['qdrant_storage'],
            collection_name=self.config['qdrant']['collection_name']
        )

        # 3. BM25 ê²€ìƒ‰ ì—”ì§„
        logger.info("Loading BM25 index...")
        self.bm25_engine = BM25SearchEngine(use_korean_tokenizer=True)

        bm25_index_path = Path(self.config['data']['cache_dir']) / 'bm25_index.pkl'
        if bm25_index_path.exists():
            self.bm25_engine.load_index(str(bm25_index_path))
        else:
            logger.warning("BM25 index not found. Please run indexing first.")

        # 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„
        self.hybrid_engine = HybridSearchEngine(
            vector_store=self.vector_store,
            bm25_engine=self.bm25_engine,
            bm25_weight=self.config['search']['bm25_weight'],
            vector_weight=self.config['search']['semantic_weight']
        )

        # 5. LLM (í”„ë¡œí† íƒ€ì…ì—ì„œëŠ” ì„ íƒì ìœ¼ë¡œ ë¡œë“œ) + ìºì‹± ì ìš©
        self.summarizer = None
        if self.llm_enabled:
            try:
                logger.info("Loading LLM (this may take a while)...")
                llm_config = LLMConfig(
                    model_name=self.config['llm']['model_name'],
                    device=self.config['llm']['device'],
                    temperature=self.config['llm']['temperature'],
                    max_tokens=self.config['llm']['max_tokens'],
                    use_vllm=False  # í”„ë¡œí† íƒ€ì…ì—ì„œëŠ” transformers ì‚¬ìš©
                )
                base_summarizer = QwenSummarizer(llm_config)
                # CachedSummarizerë¡œ ë˜í•‘í•˜ì—¬ ë™ì¼ ë¬¸ì„œ ì¬ìš”ì•½ ë°©ì§€
                self.summarizer = CachedSummarizer(base_summarizer, cache_size=500)
                logger.info("LLM loaded with caching enabled (cache_size=500)")
            except Exception as e:
                logger.warning(f"LLM loading failed (optional): {e}")
        else:
            logger.info("LLM disabled via config. Skipping LLM load.")

        # 6. ì¶”ì²œ ì‹œìŠ¤í…œ
        self.recommender = FileRecommender(
            temporal_window_hours=self.config['recommendation']['temporal_window_hours']
        )

        # 7. ì¿¼ë¦¬ íŒŒì„œ (ì‹œê°„ í‘œí˜„, í•„í„° ì¶”ì¶œ)
        self.query_parser = QueryParser()

        logger.info("All components initialized successfully!")

    def search_files(
        self,
        query: str,
        top_k: int = 5,
        include_summary: bool = True,
        include_recommendations: bool = True,
        file_type_filter: Optional[str] = None,
        sort_by: str = 'relevance'  # 'relevance', 'date_desc', 'date_asc', 'name'
    ) -> Dict:
        """
        íŒŒì¼ ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            include_summary: ìš”ì•½ í¬í•¨ ì—¬ë¶€
            include_recommendations: ì¶”ì²œ í¬í•¨ ì—¬ë¶€
            file_type_filter: íŒŒì¼ íƒ€ì… í•„í„° (None, 'pdf', 'docx', 'pptx', 'xlsx', 'image')
            sort_by: ì •ë ¬ ê¸°ì¤€ ('relevance', 'date_desc', 'date_asc', 'name')

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"Searching for: '{query}'")

        # 0. ì¿¼ë¦¬ íŒŒì‹± (ì‹œê°„ í‘œí˜„, íŒŒì¼ íƒ€ì… ìë™ ì¶”ì¶œ)
        parsed = self.query_parser.parse(query)
        search_query = parsed['cleaned_query'] or query

        # UIì—ì„œ ì§€ì •í•œ í•„í„°ê°€ ì—†ìœ¼ë©´ ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•œ í•„í„° ì‚¬ìš©
        if not file_type_filter and parsed['file_type']:
            file_type_filter = parsed['file_type']

        # ì ìš©ëœ í•„í„° ì •ë³´ (ì‚¬ìš©ìì—ê²Œ í‘œì‹œìš©)
        applied_filters = {
            'date_filter': parsed['date_filter'],
            'file_type': file_type_filter,
            'department': parsed['department']
        }

        logger.info(f"Parsed query: '{search_query}', Filters: {applied_filters}")

        # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embedder.encode_queries(search_query)

        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í•„í„° ì ìš©)
        filter_conditions = {}
        if file_type_filter:
            if file_type_filter == 'image':
                # ì´ë¯¸ì§€ëŠ” ì—¬ëŸ¬ í™•ì¥ì í¬í•¨
                filter_conditions['file_type'] = ['png', 'jpg', 'jpeg']
            else:
                filter_conditions['file_type'] = file_type_filter

        if filter_conditions:
            raw_results = self.hybrid_engine.search_with_filter(
                query=search_query,
                query_embedding=query_embedding,
                filter_conditions=filter_conditions,
                top_k=self.config['search']['top_k']
            )
        else:
            raw_results = self.hybrid_engine.search(
                query=search_query,
                query_embedding=query_embedding,
                top_k=self.config['search']['top_k'],
                final_top_k=top_k * 2  # í•„í„°ë§ í›„ ì¤„ì–´ë“¤ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ìœ ìˆê²Œ
            )

        # 2-1. íŒŒì¼ ë‹¨ìœ„ë¡œ ê²°ê³¼ ì§‘ê³„ (ì²­í¬ ì¤‘ë³µ ì œê±°)
        results = self._aggregate_results_by_file(raw_results)

        # 2-2. ë‚ ì§œ í•„í„° ì ìš© (ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ì‹œê°„ í‘œí˜„ ê¸°ë°˜)
        if parsed['date_filter']:
            results = self._apply_date_filter(results, parsed['date_filter'])

        # 2-3. ì •ë ¬ ì ìš©
        results = self._apply_sorting(results, sort_by)

        # top_k ì œí•œ
        results = results[:top_k]

        logger.info(f"Found {len(results)} results (after filters and sorting)")

        # 3. ìš”ì•½ ìƒì„± (ì„ íƒì )
        if include_summary and self.summarizer:
            logger.info("Generating summaries...")
            for result in results:
                text = result.get('text', '') or result.get('metadata', {}).get('text', '')
                if text:
                    try:
                        summary = self.summarizer.summarize(
                            text[:4000],  # ê¸¸ì´ ì œí•œ
                            style="bullet_points"
                        )
                        result['summary'] = summary
                    except Exception as e:
                        logger.warning(f"Summary generation failed: {e}")
                        result['summary'] = "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
        else:
            for result in results:
                result['summary'] = "ìš”ì•½ ë¯¸ì‚¬ìš©"

        # 4. ì—°ê´€ íŒŒì¼ ì¶”ì²œ (ì„ íƒì )
        # ë¹ˆ ê²°ê³¼ ì²˜ë¦¬: resultsê°€ ë¹„ì–´ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        if not results:
            return {
                'query': query,
                'results': [],
                'total_found': 0
            }

        if include_recommendations:
            logger.info("Generating recommendations...")
            # ì²« ë²ˆì§¸ ê²°ê³¼ì— ëŒ€í•œ ì¶”ì²œë§Œ ìƒì„± (í”„ë¡œí† íƒ€ì…)
            top_result = results[0]

            # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ë²¡í„° í¬í•¨í•˜ì—¬ ì¶”ì²œ ì •í™•ë„ í–¥ìƒ)
            all_results = self.vector_store.search(
                query_vector=query_embedding,
                top_k=50,
                with_vectors=True  # ë²¡í„° í¬í•¨í•˜ì—¬ ì¶”ì²œì— í™œìš©
            )

            # top_resultê°€ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ìœ¼ë©´ í¬í•¨ (ë²¡í„° ì¡°íšŒ ì‹œë„)
            candidate_map = {r['id']: r for r in all_results}
            if top_result['id'] not in candidate_map:
                fetched = self.vector_store.get_document(top_result['id'])
                candidate_map[top_result['id']] = {
                    'id': top_result['id'],
                    'score': top_result.get('score', 0),
                    'payload': (fetched or {}).get('payload', {}) or top_result.get('metadata', {}),
                    'vector': (fetched or {}).get('vector'),
                    'metadata': top_result.get('metadata', {})
                }

            all_results = list(candidate_map.values())

            # ì¶”ì²œ ìƒì„±
            recommendations = []
            if len(all_results) > 1:
                # ë©”íƒ€ë°ì´í„° ë˜ëŠ” payloadì—ì„œ ì •ë³´ ì¶”ì¶œ (fallback ì²˜ë¦¬)
                def get_file_info(r):
                    meta = r.get('metadata', {}) or {}
                    payload = r.get('payload', {}) or {}
                    file_id = meta.get('file_id') or payload.get('file_id')
                    if not file_id:
                        rid = r.get('id', '')
                        file_id = rid.split('_chunk_')[0] if '_chunk_' in rid else rid
                    return {
                        'id': r['id'],
                        'file_id': file_id,
                        'path': meta.get('file_path') or payload.get('file_path', ''),
                        'file_type': meta.get('file_type') or payload.get('file_type', ''),
                        'modified_time': meta.get('modified_time') or payload.get('modified_time', ''),
                        'file_name': meta.get('file_name') or payload.get('file_name', '')
                    }

                target_file = get_file_info(top_result)
                candidate_files = [get_file_info(r) for r in all_results]

                # ì„ë² ë”© ìˆ˜ì§‘ (ì¶”ì²œ ì •í™•ë„ í–¥ìƒ)
                candidate_embeddings = None
                target_embedding_for_rec = None

                # íƒ€ê²Ÿ ë¬¸ì„œ ì„ë² ë”© ì¡°íšŒ (ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì¬ì„ë² ë”©)
                target_embedding_for_rec = self._get_vector_for_result(top_result)

                # candidate_embeddings ìˆ˜ì§‘ (with_vectors=Trueë¡œ ê²€ìƒ‰í•œ ê²°ê³¼ì—ì„œ)
                try:
                    vectors_list = []
                    for r in all_results:
                        if 'vector' in r and r['vector'] is not None:
                            vectors_list.append(np.array(r['vector'], dtype=np.float32))
                        else:
                            vectors_list.append(np.zeros(self.embedder.get_embedding_dim(), dtype=np.float32))

                    if vectors_list:
                        candidate_embeddings = np.vstack(vectors_list)
                except Exception as e:
                    logger.warning(f"Failed to get embeddings for recommendations: {e}")
                    candidate_embeddings = None

                recommendations = self.recommender.recommend_similar_files(
                    target_file=target_file,
                    candidate_files=candidate_files,
                    target_embedding=target_embedding_for_rec,
                    candidate_embeddings=candidate_embeddings,
                    top_k=5
                )

            results[0]['recommendations'] = recommendations
        else:
            results[0]['recommendations'] = []

        return {
            'query': query,
            'results': results,
            'total_found': len(results),
            'applied_filters': applied_filters,
            'sort_by': sort_by
        }

    def _apply_date_filter(self, results: List[Dict], date_filter: Dict) -> List[Dict]:
        """ë‚ ì§œ í•„í„° ì ìš©"""
        if not date_filter:
            return results

        start_date = date_filter.get('start_date')
        end_date = date_filter.get('end_date')

        if not start_date or not end_date:
            return results

        filtered = []
        for result in results:
            meta = result.get('metadata', {})
            modified_time_str = meta.get('modified_time', '')

            if not modified_time_str:
                # ë‚ ì§œ ì •ë³´ ì—†ìœ¼ë©´ í¬í•¨ (í•„í„°ë§í•˜ì§€ ì•ŠìŒ)
                filtered.append(result)
                continue

            try:
                # ISO í˜•ì‹ íŒŒì‹±
                modified_time = datetime.fromisoformat(modified_time_str.replace('Z', '+00:00'))
                # timezone ì œê±°í•˜ì—¬ ë¹„êµ
                modified_time = modified_time.replace(tzinfo=None)

                if start_date <= modified_time <= end_date:
                    filtered.append(result)
            except (ValueError, TypeError):
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨
                filtered.append(result)

        return filtered

    def _apply_sorting(self, results: List[Dict], sort_by: str) -> List[Dict]:
        """ì •ë ¬ ì ìš©"""
        if not results:
            return results

        if sort_by == 'relevance':
            # ê¸°ë³¸: ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìŒ)
            return sorted(results, key=lambda r: r.get('score', 0), reverse=True)

        elif sort_by == 'date_desc':
            # ìµœì‹ ìˆœ
            def get_date(r):
                meta = r.get('metadata', {})
                time_str = meta.get('modified_time', '')
                try:
                    return datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                except:
                    return datetime.min
            return sorted(results, key=get_date, reverse=True)

        elif sort_by == 'date_asc':
            # ì˜¤ë˜ëœìˆœ
            def get_date(r):
                meta = r.get('metadata', {})
                time_str = meta.get('modified_time', '')
                try:
                    return datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                except:
                    return datetime.max
            return sorted(results, key=get_date, reverse=False)

        elif sort_by == 'name':
            # íŒŒì¼ëª…ìˆœ
            def get_name(r):
                meta = r.get('metadata', {})
                return meta.get('file_name', '') or ''
            return sorted(results, key=get_name)

        return results

    def _extract_file_id(self, result: Dict) -> str:
        """ê²°ê³¼ì—ì„œ file_id ì¶”ì¶œ (ì—†ìœ¼ë©´ chunk_id ê¸°ë°˜ ìƒì„±)"""
        meta = result.get('metadata', {}) or {}
        payload = result.get('payload', {}) or {}
        file_id = meta.get('file_id') or payload.get('file_id')

        if not file_id:
            rid = result.get('id', '')
            file_id = rid.split('_chunk_')[0] if '_chunk_' in rid else rid

        return file_id

    def _aggregate_results_by_file(self, results: List[Dict]) -> List[Dict]:
        """ì²­í¬ ê²°ê³¼ë¥¼ íŒŒì¼ ë‹¨ìœ„ë¡œ ì§‘ê³„ (ìµœê³  ì ìˆ˜ ì²­í¬ë§Œ ìœ ì§€)"""
        best_by_file = {}

        for res in results:
            file_id = self._extract_file_id(res)
            meta = res.get('metadata', {}) or {}
            payload = res.get('payload', {}) or {}

            merged_meta = {**payload, **meta}
            merged_meta['file_id'] = file_id

            candidate = {
                'id': res.get('id'),
                'score': res.get('score', 0.0),
                'text': res.get('text', '') or merged_meta.get('text', ''),
                'metadata': merged_meta,
                'payload': res.get('payload', {}) or merged_meta,
                'best_chunk_id': res.get('id')
            }

            prev = best_by_file.get(file_id)
            if (prev is None) or (candidate['score'] > prev['score']):
                best_by_file[file_id] = candidate

        aggregated = list(best_by_file.values())
        aggregated.sort(key=lambda r: r['score'], reverse=True)
        return aggregated

    def _get_vector_for_result(self, result: Dict) -> np.ndarray:
        """ê²€ìƒ‰ ê²°ê³¼(ëŒ€í‘œ ì²­í¬)ì— ëŒ€ì‘í•˜ëŠ” ë²¡í„° ì¡°íšŒ ë˜ëŠ” ì¬ì„ë² ë”©"""
        chunk_id = result.get('best_chunk_id') or result.get('id')
        vector = None

        try:
            doc = self.vector_store.get_document(chunk_id)
            if doc and doc.get('vector') is not None:
                vector = np.array(doc['vector'], dtype=np.float32)
        except Exception as e:
            logger.debug(f"Vector fetch failed for {chunk_id}: {e}")

        if vector is None:
            text = result.get('text') or result.get('metadata', {}).get('text', '')
            if text:
                try:
                    emb = self.embedder.encode_documents(
                        [text],
                        include_sparse=False
                    )
                    vector = emb['dense_vecs'][0]
                except Exception as e:
                    logger.debug(f"Re-embedding failed for {chunk_id}: {e}")

        if vector is None:
            vector = np.zeros(self.embedder.get_embedding_dim(), dtype=np.float32)

        return vector

    def detect_duplicates(
        self,
        similarity_threshold: float = 0.95,
        top_k: int = 50
    ) -> List[Dict]:
        """
        ì¤‘ë³µ ë¬¸ì„œ íƒì§€ (í•´ì‹œ + ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜)

        Args:
            similarity_threshold: ì¤‘ë³µ íŒë‹¨ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.95 = 95% ìœ ì‚¬)
            top_k: ê²€ì‚¬í•  ë¬¸ì„œ ìˆ˜

        Returns:
            ì¤‘ë³µ ë¬¸ì„œ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸
            [{'original': {...}, 'duplicates': [{...}, ...], 'similarity': float}]
        """
        logger.info(f"Detecting duplicate documents (threshold: {similarity_threshold})")

        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        all_docs = []
        try:
            # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ (ì„ì˜ ë²¡í„°ë¡œ ì „ì²´ ì¡°íšŒ)
            dummy_vector = np.zeros(self.embedder.get_embedding_dim(), dtype=np.float32)
            all_docs = self.vector_store.search(
                query_vector=dummy_vector,
                top_k=top_k,
                with_vectors=True
            )
        except Exception as e:
            logger.error(f"Failed to fetch documents for duplicate detection: {e}")
            return []

        if len(all_docs) < 2:
            return []

        # íŒŒì¼ ë‹¨ìœ„ë¡œ ì§‘ê³„ (ì²­í¬ ì¤‘ë³µ ì œê±°)
        file_docs = {}
        for doc in all_docs:
            file_id = self._extract_file_id(doc)
            if file_id not in file_docs:
                file_docs[file_id] = doc

        docs_list = list(file_docs.values())

        # ì½˜í…ì¸  í•´ì‹œ ê³„ì‚°
        content_hashes = {}
        for doc in docs_list:
            text = doc.get('text', '') or doc.get('payload', {}).get('text', '')
            if text:
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                content_hashes[doc['id']] = text_hash

        # ì¤‘ë³µ ê·¸ë£¹ íƒì§€
        duplicate_groups = []
        checked_ids = set()

        for i, doc in enumerate(docs_list):
            if doc['id'] in checked_ids:
                continue

            doc_vector = doc.get('vector')
            if doc_vector is None:
                continue

            doc_vector = np.array(doc_vector, dtype=np.float32)
            doc_hash = content_hashes.get(doc['id'])

            duplicates = []

            for j, other_doc in enumerate(docs_list[i+1:], i+1):
                if other_doc['id'] in checked_ids:
                    continue

                other_vector = other_doc.get('vector')
                if other_vector is None:
                    continue

                other_vector = np.array(other_vector, dtype=np.float32)
                other_hash = content_hashes.get(other_doc['id'])

                # 1. í•´ì‹œ ë¹„êµ (ì™„ì „ ì¼ì¹˜)
                is_hash_match = doc_hash and other_hash and doc_hash == other_hash

                # 2. ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                similarity = 0.0
                norm_a = np.linalg.norm(doc_vector)
                norm_b = np.linalg.norm(other_vector)
                if norm_a > 0 and norm_b > 0:
                    similarity = float(np.dot(doc_vector, other_vector) / (norm_a * norm_b))

                # ì¤‘ë³µ íŒë‹¨
                if is_hash_match or similarity >= similarity_threshold:
                    other_meta = other_doc.get('payload', {}) or other_doc.get('metadata', {})
                    duplicates.append({
                        'id': other_doc['id'],
                        'file_name': other_meta.get('file_name', 'Unknown'),
                        'file_path': other_meta.get('file_path', 'N/A'),
                        'similarity': 1.0 if is_hash_match else similarity,
                        'match_type': 'hash' if is_hash_match else 'semantic'
                    })
                    checked_ids.add(other_doc['id'])

            if duplicates:
                doc_meta = doc.get('payload', {}) or doc.get('metadata', {})
                duplicate_groups.append({
                    'original': {
                        'id': doc['id'],
                        'file_name': doc_meta.get('file_name', 'Unknown'),
                        'file_path': doc_meta.get('file_path', 'N/A')
                    },
                    'duplicates': duplicates,
                    'count': len(duplicates)
                })
                checked_ids.add(doc['id'])

        logger.info(f"Found {len(duplicate_groups)} duplicate groups")
        return duplicate_groups

    def answer_question(self, question: str, context: str) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸

        Returns:
            LLM ì‘ë‹µ
        """
        if not self.summarizer:
            return "LLMì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ì§ˆë¬¸ ë‹µë³€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        try:
            # QwenSummarizer.answer_question(context, question) ìˆœì„œì— ë§ì¶¤
            response = self.summarizer.answer_question(context, question)
            return response
        except Exception as e:
            logger.error(f"Question answering failed: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


class GradioChatInterface:
    """Gradio ì±—ë´‡ UI - ëŒ€í™”í˜• íŒŒì¼ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, pipeline: AIAgentPipeline):
        self.pipeline = pipeline
        self.conversation_history = []
        self.last_search_results = None

    def _parse_command(self, message: str) -> Tuple[str, Dict]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ëª…ë ¹ì–´ íŒŒì‹±

        Returns:
            (command_type, params)
            command_type: 'search', 'duplicate', 'recommend', 'help', 'question'
        """
        message_lower = message.lower().strip()

        # ì¤‘ë³µ ë¬¸ì„œ íƒì§€ ëª…ë ¹
        if any(kw in message_lower for kw in ['ì¤‘ë³µ', 'ì¤‘ë³µ íƒì§€', 'ì¤‘ë³µ ê²€ì‚¬', 'duplicate']):
            return 'duplicate', {}

        # ë„ì›€ë§ ëª…ë ¹
        if any(kw in message_lower for kw in ['/help', 'ë„ì›€ë§', 'ì‚¬ìš©ë²•']):
            return 'help', {}

        # ì¶”ì²œ ëª…ë ¹ (ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜)
        if any(kw in message_lower for kw in ['ì¶”ì²œ', 'ìœ ì‚¬í•œ íŒŒì¼', 'similar', 'ì—°ê´€']):
            if self.last_search_results:
                return 'recommend', {}

        # LLM ì§ˆë¬¸ (ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆê³ , '?' ë˜ëŠ” ì§ˆë¬¸í˜• ë¬¸ì¥ì¸ ê²½ìš°)
        if self.last_search_results and self.pipeline.summarizer:
            if '?' in message or any(kw in message_lower for kw in ['ë­ì•¼', 'ë­”ê°€ìš”', 'ì•Œë ¤', 'ì„¤ëª…', 'ì–´ë–»ê²Œ']):
                return 'question', {'question': message}

        # ê¸°ë³¸: ê²€ìƒ‰
        return 'search', {'query': message}

    def chat_response(
        self,
        message: str,
        history: List[Dict],
        top_k: int,
        include_summary: bool,
        include_recommendations: bool,
        show_explanation: bool,
        file_type_filter: str,
        sort_by: str
    ) -> Tuple[str, List[Dict]]:
        """
        ì±—ë´‡ ì‘ë‹µ ìƒì„±

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (Gradio 6.x í˜•ì‹)
            top_k: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            include_summary: ìš”ì•½ í¬í•¨ ì—¬ë¶€
            include_recommendations: ì¶”ì²œ í¬í•¨ ì—¬ë¶€
            show_explanation: ê²€ìƒ‰ ì„¤ëª… í‘œì‹œ ì—¬ë¶€
            file_type_filter: íŒŒì¼ íƒ€ì… í•„í„°
            sort_by: ì •ë ¬ ê¸°ì¤€

        Returns:
            (ì‘ë‹µ í…ìŠ¤íŠ¸, ì—…ë°ì´íŠ¸ëœ íˆìŠ¤í† ë¦¬)
        """
        if not message.strip():
            return "", history

        try:
            command, params = self._parse_command(message)

            if command == 'help':
                response = self._get_help_message()

            elif command == 'duplicate':
                response = self._handle_duplicate_detection()

            elif command == 'recommend':
                response = self._handle_recommendation()

            elif command == 'question':
                response = self._handle_question(params['question'])

            else:  # search
                response = self._handle_search(
                    params['query'],
                    top_k,
                    include_summary,
                    include_recommendations,
                    show_explanation,
                    file_type_filter,
                    sort_by
                )

            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (Gradio 6.x í˜•ì‹)
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": response})
            return "", history

        except Exception as e:
            logger.error(f"Chat error: {e}", exc_info=True)
            error_response = self._format_user_friendly_error(e)
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": error_response})
            return "", history

    def _format_user_friendly_error(self, error: Exception) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±"""
        error_str = str(error).lower()

        if 'qdrant' in error_str or 'connection' in error_str:
            return "âš ï¸ **ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜**\n\nê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nê´€ë¦¬ìì—ê²Œ ë¬¸ì˜ê°€ í•„ìš”í•œ ê²½ìš° IT ì§€ì›íŒ€ì— ì—°ë½í•´ì£¼ì„¸ìš”."

        elif 'index' in error_str or 'bm25' in error_str:
            return "âš ï¸ **ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘**\n\nê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìê°€ ì¸ë±ì‹±ì„ ì™„ë£Œí•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        elif 'memory' in error_str or 'cuda' in error_str or 'gpu' in error_str:
            return "âš ï¸ **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±**\n\ní˜„ì¬ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, ìš”ì•½ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ê²€ìƒ‰í•´ë³´ì„¸ìš”."

        elif 'timeout' in error_str:
            return "âš ï¸ **ìš”ì²­ ì‹œê°„ ì´ˆê³¼**\n\nê²€ìƒ‰ ìš”ì²­ì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•˜ê±°ë‚˜, ê²°ê³¼ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        elif 'file' in error_str and 'not found' in error_str:
            return "âš ï¸ **íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ**\n\nìš”ì²­í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì´ë™ë˜ì—ˆê±°ë‚˜ ì‚­ì œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        else:
            return f"âš ï¸ **ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**\n\nì£„ì†¡í•©ë‹ˆë‹¤. ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\në‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê³ , ë¬¸ì œê°€ ì§€ì†ë˜ë©´ IT ì§€ì›íŒ€ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”.\n\n(ì˜¤ë¥˜ ì½”ë“œ: {type(error).__name__})"

    def _handle_search(
        self,
        query: str,
        top_k: int,
        include_summary: bool,
        include_recommendations: bool,
        show_explanation: bool,
        file_type_filter: str,
        sort_by: str
    ) -> str:
        """ê²€ìƒ‰ ì²˜ë¦¬"""
        # 'ì „ì²´' ì„ íƒ ì‹œ Noneìœ¼ë¡œ ë³€í™˜
        actual_filter = None if file_type_filter == 'ì „ì²´' else file_type_filter

        search_result = self.pipeline.search_files(
            query=query,
            top_k=top_k,
            include_summary=include_summary,
            include_recommendations=include_recommendations,
            file_type_filter=actual_filter,
            sort_by=sort_by
        )

        # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (í›„ì† ì§ˆë¬¸ìš©)
        self.last_search_results = search_result

        return self._format_search_results(search_result, show_explanation, include_recommendations)

    def _handle_duplicate_detection(self) -> str:
        """ì¤‘ë³µ ë¬¸ì„œ íƒì§€ ì²˜ë¦¬"""
        duplicates = self.pipeline.detect_duplicates(
            similarity_threshold=0.95,
            top_k=100
        )

        return self._format_duplicates(duplicates)

    def _handle_recommendation(self) -> str:
        """ì¶”ì²œ ì²˜ë¦¬ (ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜)"""
        if not self.last_search_results or not self.last_search_results['results']:
            return "ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ìƒ‰ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."

        top_result = self.last_search_results['results'][0]
        recommendations = top_result.get('recommendations', [])

        return self._format_recommendations(recommendations)

    def _handle_question(self, question: str) -> str:
        """ì§ˆë¬¸ ë‹µë³€ ì²˜ë¦¬ (LLM ì‚¬ìš©)"""
        if not self.last_search_results or not self.last_search_results['results']:
            return "ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ìƒ‰ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, result in enumerate(self.last_search_results['results'][:3]):
            file_name = result.get('metadata', {}).get('file_name', 'Unknown')
            text = result.get('text', '')[:1000]
            context_parts.append(f"[ë¬¸ì„œ {i+1}: {file_name}]\n{text}")

        context = "\n\n".join(context_parts)

        # LLM ì‘ë‹µ ìƒì„±
        response = self.pipeline.answer_question(question, context)

        return f"**ì§ˆë¬¸:** {question}\n\n**ë‹µë³€:**\n{response}"

    def _get_help_message(self) -> str:
        """ë„ì›€ë§ ë©”ì‹œì§€"""
        return """## ğŸ“– ì‚¬ë‚´ íŒŒì¼ ê²€ìƒ‰ AI Agent ì‚¬ìš©ë²•

### ğŸ” ê¸°ë³¸ ê²€ìƒ‰
ìì—°ì–´ë¡œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì‹œìŠ¤í…œì´ í‚¤ì›Œë“œì™€ ì˜ë¯¸ë¥¼ ëª¨ë‘ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ì¤ë‹ˆë‹¤.

### â° ì‹œê°„ í‘œí˜„ (ìë™ ì¸ì‹)
ê²€ìƒ‰ì–´ì— ì‹œê°„ í‘œí˜„ì„ í¬í•¨í•˜ë©´ ìë™ìœ¼ë¡œ ë‚ ì§œ í•„í„°ê°€ ì ìš©ë©ë‹ˆë‹¤.
- **ìƒëŒ€ ì‹œê°„**: "ì‘ë…„", "ì§€ë‚œë‹¬", "ì´ë²ˆ ì£¼", "ì–´ì œ", "ìµœê·¼ 3ê°œì›”"
- **ì ˆëŒ€ ì‹œê°„**: "2024ë…„", "2023ë…„ ìƒë°˜ê¸°", "1ë¶„ê¸°"
- **ì˜ˆì‹œ**: "ì‘ë…„ ì•ˆì „ ì ê²€ ë³´ê³ ì„œ" â†’ 2025ë…„ ë¬¸ì„œë§Œ ê²€ìƒ‰

### ğŸ“ íŒŒì¼ íƒ€ì… (ìë™ ì¸ì‹)
ê²€ìƒ‰ì–´ì— íŒŒì¼ íƒ€ì…ì„ í¬í•¨í•˜ë©´ ìë™ìœ¼ë¡œ í•„í„°ë§ë©ë‹ˆë‹¤.
- "PDF íŒŒì¼", "ì›Œë“œ ë¬¸ì„œ", "ì—‘ì…€", "íŒŒì›Œí¬ì¸íŠ¸", "ì´ë¯¸ì§€"
- **ì˜ˆì‹œ**: "ë§ˆì¼€íŒ…íŒ€ PDF ë¬¸ì„œ" â†’ PDF íŒŒì¼ë§Œ ê²€ìƒ‰

### ğŸ¢ ë¶€ì„œëª… (ìë™ ì¸ì‹)
ë¶€ì„œëª…ì´ í¬í•¨ë˜ë©´ í•´ë‹¹ ë¶€ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ìš°ì„  ê²€ìƒ‰í•©ë‹ˆë‹¤.
- ê¸°íšíŒ€, ê°œë°œíŒ€, ë§ˆì¼€íŒ…íŒ€, ì˜ì—…íŒ€, ì¸ì‚¬íŒ€, ì¬ë¬´íŒ€, ë””ìì¸íŒ€, í’ˆì§ˆê´€ë¦¬íŒ€

### âš™ï¸ ê²€ìƒ‰ ì„¤ì • (ì˜¤ë¥¸ìª½ íŒ¨ë„)
- **íŒŒì¼ íƒ€ì… í•„í„°**: íŠ¹ì • íŒŒì¼ í˜•ì‹ë§Œ ê²€ìƒ‰
- **ì •ë ¬ ê¸°ì¤€**: ê´€ë ¨ë„ìˆœ, ìµœì‹ ìˆœ, ì˜¤ë˜ëœìˆœ, íŒŒì¼ëª…ìˆœ
- **ê²€ìƒ‰ ì„¤ëª… í‘œì‹œ**: ì™œ ì´ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ ê·¼ê±° í‘œì‹œ
- **ì—°ê´€ íŒŒì¼ ì¶”ì²œ**: ê²€ìƒ‰ ê²°ê³¼ì™€ ìœ ì‚¬í•œ íŒŒì¼ ì¶”ì²œ
- **ìš”ì•½ ìƒì„±**: LLMì´ ë¬¸ì„œ ë‚´ìš©ì„ ìš”ì•½ (í™œì„±í™” í•„ìš”)

### ğŸ”§ íŠ¹ìˆ˜ ëª…ë ¹
- **"ì¤‘ë³µ ê²€ì‚¬"**: ìœ ì‚¬í•˜ê±°ë‚˜ ë™ì¼í•œ ë¬¸ì„œ ê·¸ë£¹ì„ ì°¾ì•„ì¤ë‹ˆë‹¤
- **"ì¶”ì²œ" / "ìœ ì‚¬í•œ íŒŒì¼"**: ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì—°ê´€ íŒŒì¼ ì¶”ì²œ
- **"/help"**: ì´ ë„ì›€ë§ í‘œì‹œ

### ğŸ’¬ í›„ì† ì§ˆë¬¸ (LLM í™œì„±í™” ì‹œ)
ê²€ìƒ‰ í›„ ê²°ê³¼ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì´ ë­ì•¼?"
- "ROIê°€ ì–¼ë§ˆë¼ê³  í–ˆì–´?"

### ğŸ’¡ ê²€ìƒ‰ íŒ
1. **êµ¬ì²´ì ìœ¼ë¡œ**: "ë³´ê³ ì„œ" ë³´ë‹¤ "2024ë…„ ìƒë°˜ê¸° ë§¤ì¶œ ë³´ê³ ì„œ"
2. **ì‹œê°„ í™œìš©**: "ì‘ë…„ íšŒì˜ë¡", "ìµœê·¼ 1ì£¼ì¼ ê³„íšì„œ"
3. **íŒŒì¼ íƒ€ì… ì§€ì •**: "ì—‘ì…€ë¡œ ëœ ì˜ˆì‚° ìë£Œ"
4. **ë¶€ì„œ ì–¸ê¸‰**: "ë§ˆì¼€íŒ…íŒ€ ìº í˜ì¸ ë¶„ì„"
"""

    def _highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            keywords: í•˜ì´ë¼ì´íŠ¸í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            í‚¤ì›Œë“œê°€ **ê°•ì¡°**ëœ í…ìŠ¤íŠ¸
        """
        if not keywords or not text:
            return text

        highlighted = text
        for keyword in keywords:
            if len(keyword) < 2:  # ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œëŠ” ì œì™¸
                continue
            # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë§¤ì¹­, ì›ë³¸ ì¼€ì´ìŠ¤ ìœ ì§€í•˜ë©´ì„œ ê°•ì¡°
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            highlighted = pattern.sub(lambda m: f"**{m.group()}**", highlighted)

        return highlighted

    def _format_search_results(
        self,
        search_result: Dict,
        show_explanation: bool,
        include_recommendations: bool
    ) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… (ì„¤ëª… í¬í•¨)"""
        if not search_result['results']:
            return f"'{search_result['query']}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        output = f"## ê²€ìƒ‰ ê²°ê³¼ (ì´ {search_result['total_found']}ê°œ)\n\n"

        for i, result in enumerate(search_result['results']):
            meta = result.get('metadata', {})
            file_name = meta.get('file_name', 'Unknown')
            file_path = meta.get('file_path', 'N/A')
            file_type = meta.get('file_type', 'N/A')
            score = result.get('score', 0)

            # ë§¤ì¹­ í‚¤ì›Œë“œ ì¶”ì¶œ (í•˜ì´ë¼ì´íŠ¸ìš©)
            matched_keywords = []
            if 'explanation' in result:
                matched_keywords = result['explanation'].get('matched_keywords', [])

            output += f"### {i+1}. {file_name}\n"
            output += f"- **ê²½ë¡œ:** `{file_path}`\n"
            output += f"- **íƒ€ì…:** {file_type}\n"
            output += f"- **í†µí•© ì ìˆ˜:** {score:.4f}\n"

            # ì›ë¬¸ ë°”ë¡œê°€ê¸° ë²„íŠ¼ (íŒŒì¼ ê²½ë¡œ ë§í¬)
            if file_path and file_path != 'N/A':
                # file:// í”„ë¡œí† ì½œë¡œ ë¡œì»¬ íŒŒì¼ ë§í¬ ìƒì„±
                file_link = f"file://{file_path}"
                output += f"- ğŸ“‚ [ì›ë¬¸ ì—´ê¸°]({file_link})\n"

            # ê²€ìƒ‰ ì„¤ëª… (ê·¼ê±°) í‘œì‹œ
            if show_explanation and 'explanation' in result:
                exp = result['explanation']
                output += "\n**ê²€ìƒ‰ ê·¼ê±°:**\n"

                # ë§¤ì¹­ íƒ€ì…
                search_types = exp.get('search_type', [])
                type_str = ', '.join(['í‚¤ì›Œë“œ' if t == 'keyword' else 'ì˜ë¯¸' for t in search_types])
                output += f"- ë§¤ì¹­ ë°©ì‹: {type_str or 'N/A'}\n"

                # ì ìˆ˜ ë¶„í•´
                bm25_score = exp.get('bm25_score', 0)
                vector_score = exp.get('vector_score', 0)
                output += f"- BM25(í‚¤ì›Œë“œ) ì ìˆ˜: {bm25_score:.4f}\n"
                output += f"- ë²¡í„°(ì˜ë¯¸) ì ìˆ˜: {vector_score:.4f}\n"

                # ë§¤ì¹­ í‚¤ì›Œë“œ (í•˜ì´ë¼ì´íŠ¸ëœ í˜•íƒœë¡œ í‘œì‹œ)
                if matched_keywords:
                    highlighted_keywords = [f"**{kw}**" for kw in matched_keywords[:5]]
                    output += f"- ë§¤ì¹­ í‚¤ì›Œë“œ: {', '.join(highlighted_keywords)}\n"

            # ìš”ì•½
            if 'summary' in result and result['summary'] != "ìš”ì•½ ë¯¸ì‚¬ìš©":
                output += f"\n**ìš”ì•½:** {result['summary']}\n"

            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ì ìš©)
            text_preview = result.get('text', '')[:300]
            if not text_preview:
                text_preview = meta.get('text', '')[:300]
            if text_preview:
                # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ì ìš©
                highlighted_preview = self._highlight_keywords(text_preview, matched_keywords)
                output += f"\n**ë¯¸ë¦¬ë³´ê¸°:** {highlighted_preview}...\n"

            output += "\n---\n"

        # ì¶”ì²œ íŒŒì¼ (ì²« ë²ˆì§¸ ê²°ê³¼ì— ëŒ€í•´)
        if include_recommendations and search_result['results']:
            recommendations = search_result['results'][0].get('recommendations', [])
            if recommendations:
                output += "\n## ì—°ê´€ íŒŒì¼ ì¶”ì²œ\n"
                for i, rec in enumerate(recommendations[:3]):
                    rec_path = rec.get('path', 'N/A')
                    rec_name = rec.get('file_name', 'Unknown')
                    rec_score = rec.get('recommendation_score', 0)
                    output += f"- **{rec_name}** (ì ìˆ˜: {rec_score:.2f})"
                    if rec_path and rec_path != 'N/A':
                        output += f" - ğŸ“‚ [ì—´ê¸°](file://{rec_path})"
                    output += "\n"

        return output

    def _format_duplicates(self, duplicate_groups: List[Dict]) -> str:
        """ì¤‘ë³µ ë¬¸ì„œ í¬ë§·íŒ…"""
        if not duplicate_groups:
            return "ì¤‘ë³µ ë¬¸ì„œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        output = f"## ì¤‘ë³µ ë¬¸ì„œ íƒì§€ ê²°ê³¼\n\n"
        output += f"ì´ **{len(duplicate_groups)}ê°œ** ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬\n\n"

        for i, group in enumerate(duplicate_groups[:10]):  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
            original = group['original']
            duplicates = group['duplicates']

            output += f"### ê·¸ë£¹ {i+1}: {original['file_name']}\n"
            output += f"- **ì›ë³¸:** `{original['file_path']}`\n"
            output += f"- **ì¤‘ë³µ ìˆ˜:** {group['count']}ê°œ\n\n"

            output += "| íŒŒì¼ëª… | ìœ ì‚¬ë„ | íƒì§€ ë°©ì‹ |\n"
            output += "|--------|--------|----------|\n"

            for dup in duplicates[:5]:  # ê° ê·¸ë£¹ë‹¹ 5ê°œê¹Œì§€
                similarity_pct = dup['similarity'] * 100
                match_type = 'í•´ì‹œ ì¼ì¹˜' if dup['match_type'] == 'hash' else 'ì˜ë¯¸ ìœ ì‚¬'
                output += f"| {dup['file_name'][:30]} | {similarity_pct:.1f}% | {match_type} |\n"

            output += "\n"

        return output

    def _format_recommendations(self, recommendations: List[Dict]) -> str:
        """ì¶”ì²œ íŒŒì¼ í¬ë§·íŒ…"""
        if not recommendations:
            return "ì¶”ì²œí•  ì—°ê´€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

        output = "## ì—°ê´€ íŒŒì¼ ì¶”ì²œ\n\n"

        for i, rec in enumerate(recommendations):
            output += f"### {i+1}. {rec.get('file_name', 'Unknown')}\n"
            output += f"- **ê²½ë¡œ:** `{rec.get('path', 'N/A')}`\n"
            output += f"- **ì¶”ì²œ ì ìˆ˜:** {rec.get('recommendation_score', 0):.4f}\n"

            # ìœ ì‚¬ë„ ì„¸ë¶€ì‚¬í•­
            breakdown = rec.get('similarity_breakdown', {})
            output += "- **ìœ ì‚¬ë„ ìƒì„¸:**\n"
            output += f"  - ë‚´ìš© ìœ ì‚¬ë„: {breakdown.get('vector', 0):.2f}\n"
            output += f"  - ì‹œê°„ ì—°ê´€ì„±: {breakdown.get('temporal', 0):.2f}\n"
            output += f"  - ê²½ë¡œ ìœ ì‚¬ë„: {breakdown.get('path', 0):.2f}\n"
            output += f"  - íƒ€ì… ì¼ì¹˜: {breakdown.get('type', 0):.2f}\n\n"

        return output

    def create_ui(self):
        """Gradio ì±—ë´‡ UI ìƒì„±"""
        llm_available = self.pipeline.summarizer is not None

        with gr.Blocks(title="ì‚¬ë‚´ íŒŒì¼ ê²€ìƒ‰ AI Agent") as demo:
            gr.Markdown("# ğŸ” ì‚¬ë‚´ ë„¤íŠ¸ì›Œí¬ ë“œë¼ì´ë¸Œ íŒŒì¼ ê²€ìƒ‰ AI Agent")
            gr.Markdown("ìì—°ì–´ë¡œ íŒŒì¼ì„ ê²€ìƒ‰í•˜ê³ , ê´€ë ¨ ë¬¸ì„œë¥¼ ì¶”ì²œë°›ìœ¼ì„¸ìš”. ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¡œ í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            # ìƒíƒœ í‘œì‹œ
            status_text = "âœ… LLM í™œì„±í™” (ìš”ì•½/ì§ˆë¬¸ë‹µë³€ ê°€ëŠ¥)" if llm_available else "âš ï¸ LLM ë¹„í™œì„±í™” (ìš”ì•½/ì§ˆë¬¸ë‹µë³€ ë¶ˆê°€)"
            status_color = "green" if llm_available else "orange"
            gr.Markdown(f"> **ì‹œìŠ¤í…œ ìƒíƒœ:** <span style='color:{status_color}'>{status_text}</span>")

            with gr.Row():
                # ë©”ì¸ ì±„íŒ… ì˜ì—­
                with gr.Column(scale=3):
                    chatbot = gr.Chatbot(
                        label="ëŒ€í™”",
                        height=500
                    )

                    with gr.Row():
                        msg_input = gr.Textbox(
                            label="ë©”ì‹œì§€ ì…ë ¥",
                            placeholder="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: 'ì‘ë…„ ì•ˆì „ ì ê²€ ë³´ê³ ì„œ', 'ë§ˆì¼€íŒ…íŒ€ PDF íŒŒì¼', 'ì¤‘ë³µ ê²€ì‚¬'",
                            lines=2,
                            scale=4
                        )
                        send_btn = gr.Button("ğŸ” ê²€ìƒ‰", variant="primary", scale=1)

                # ì„¤ì • íŒ¨ë„
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ ê²€ìƒ‰ ì„¤ì •")

                    top_k_slider = gr.Slider(
                        minimum=1,
                        maximum=10,
                        value=5,
                        step=1,
                        label="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜"
                    )

                    # íŒŒì¼ íƒ€ì… í•„í„° (ì‹ ê·œ)
                    file_type_filter = gr.Dropdown(
                        choices=["ì „ì²´", "pdf", "docx", "pptx", "xlsx", "image"],
                        value="ì „ì²´",
                        label="ğŸ“ íŒŒì¼ íƒ€ì… í•„í„°",
                        info="íŠ¹ì • íŒŒì¼ íƒ€ì…ë§Œ ê²€ìƒ‰"
                    )

                    # ì •ë ¬ ì˜µì…˜ (ì‹ ê·œ)
                    sort_by = gr.Dropdown(
                        choices=[
                            ("ê´€ë ¨ë„ìˆœ", "relevance"),
                            ("ìµœì‹ ìˆœ", "date_desc"),
                            ("ì˜¤ë˜ëœìˆœ", "date_asc"),
                            ("íŒŒì¼ëª…ìˆœ", "name")
                        ],
                        value="relevance",
                        label="ğŸ“Š ì •ë ¬ ê¸°ì¤€"
                    )

                    gr.Markdown("---")
                    gr.Markdown("### ğŸ“‹ í‘œì‹œ ì˜µì…˜")

                    show_explanation = gr.Checkbox(
                        label="ê²€ìƒ‰ ì„¤ëª… í‘œì‹œ (ë§¤ì¹­ ê·¼ê±°)",
                        value=True
                    )

                    include_recommendations = gr.Checkbox(
                        label="ì—°ê´€ íŒŒì¼ ì¶”ì²œ",
                        value=True
                    )

                    summary_label = "ğŸ“ ìš”ì•½ ìƒì„±" if llm_available else "ğŸ“ ìš”ì•½ ìƒì„± (ë¹„í™œì„±í™”)"
                    include_summary = gr.Checkbox(
                        label=summary_label,
                        value=False,
                        interactive=llm_available
                    )

                    gr.Markdown("---")
                    gr.Markdown("### ğŸš€ ë¹ ë¥¸ ëª…ë ¹")

                    help_btn = gr.Button("â“ ë„ì›€ë§", size="sm")
                    duplicate_btn = gr.Button("ğŸ”„ ì¤‘ë³µ ë¬¸ì„œ íƒì§€", size="sm")
                    clear_btn = gr.Button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", size="sm", variant="secondary")

            # ì˜ˆì‹œ ì§ˆì˜
            gr.Markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆì˜")
            gr.Markdown("ì‹œê°„ í‘œí˜„, íŒŒì¼ íƒ€ì…, ë¶€ì„œëª…ì„ í¬í•¨í•˜ë©´ ìë™ìœ¼ë¡œ í•„í„°ë§ë©ë‹ˆë‹¤.")
            gr.Examples(
                examples=[
                    ["ì‘ë…„ ì•ˆì „ ì ê²€ ë³´ê³ ì„œ"],
                    ["ë§ˆì¼€íŒ…íŒ€ PDF ë¬¸ì„œ"],
                    ["2024ë…„ ìƒë°˜ê¸° ë§¤ì¶œ ì‹¤ì "],
                    ["ìµœê·¼ 3ê°œì›” íšŒì˜ë¡"],
                    ["ê³ ê° ë§Œì¡±ë„ í–¥ìƒ ì „ëµ"],
                    ["ì¤‘ë³µ ê²€ì‚¬"],
                    ["/help"]
                ],
                inputs=msg_input
            )

            # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
            def submit_message(message, history, top_k, include_summary, include_recommendations, show_explanation, file_type_filter, sort_by):
                return self.chat_response(
                    message, history, top_k, include_summary, include_recommendations, show_explanation, file_type_filter, sort_by
                )

            # ì „ì†¡ ë²„íŠ¼ í´ë¦­
            send_btn.click(
                fn=submit_message,
                inputs=[msg_input, chatbot, top_k_slider, include_summary, include_recommendations, show_explanation, file_type_filter, sort_by],
                outputs=[msg_input, chatbot]
            )

            # Enter í‚¤ë¡œ ì „ì†¡
            msg_input.submit(
                fn=submit_message,
                inputs=[msg_input, chatbot, top_k_slider, include_summary, include_recommendations, show_explanation, file_type_filter, sort_by],
                outputs=[msg_input, chatbot]
            )

            # ë„ì›€ë§ ë²„íŠ¼
            def show_help(history):
                help_msg = self._get_help_message()
                history.append({"role": "user", "content": "ë„ì›€ë§"})
                history.append({"role": "assistant", "content": help_msg})
                return history

            help_btn.click(
                fn=show_help,
                inputs=[chatbot],
                outputs=[chatbot]
            )

            # ì¤‘ë³µ íƒì§€ ë²„íŠ¼
            def run_duplicate(history):
                response = self._handle_duplicate_detection()
                history.append({"role": "user", "content": "ì¤‘ë³µ ë¬¸ì„œ íƒì§€"})
                history.append({"role": "assistant", "content": response})
                return history

            duplicate_btn.click(
                fn=run_duplicate,
                inputs=[chatbot],
                outputs=[chatbot]
            )

            # ëŒ€í™” ì´ˆê¸°í™”
            def clear_chat():
                self.last_search_results = None
                return []

            clear_btn.click(
                fn=clear_chat,
                outputs=[chatbot]
            )

        return demo


# ê¸°ì¡´ ë‹¨ì¼ ê²€ìƒ‰ UI (í•˜ìœ„ í˜¸í™˜ì„±)
class GradioInterface:
    """Gradio ë‹¨ì¼ ê²€ìƒ‰ UI (ë ˆê±°ì‹œ)"""

    def __init__(self, pipeline: AIAgentPipeline):
        self.pipeline = pipeline

    def search_interface(self, query: str, top_k: int, include_summary: bool, include_recommendations: bool):
        if not query.strip():
            return "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""

        try:
            search_result = self.pipeline.search_files(
                query=query,
                top_k=top_k,
                include_summary=include_summary,
                include_recommendations=include_recommendations
            )

            output = f"# ê²€ìƒ‰ ê²°ê³¼ (ì´ {search_result['total_found']}ê°œ)\n\n"
            for i, result in enumerate(search_result['results']):
                meta = result.get('metadata', {})
                output += f"## {i+1}. {meta.get('file_name', 'Unknown')}\n"
                output += f"**ê²½ë¡œ:** `{meta.get('file_path', 'N/A')}`\n"
                output += f"**ì ìˆ˜:** {result['score']:.4f}\n\n"

            recommendations_output = ""
            if include_recommendations and search_result['results']:
                recs = search_result['results'][0].get('recommendations', [])
                if recs:
                    recommendations_output = "# ì¶”ì²œ íŒŒì¼\n" + "\n".join([f"- {r['file_name']}" for r in recs])

            return output, recommendations_output

        except Exception as e:
            return f"ì˜¤ë¥˜: {e}", ""

    def create_ui(self):
        with gr.Blocks(title="íŒŒì¼ ê²€ìƒ‰") as demo:
            gr.Markdown("# íŒŒì¼ ê²€ìƒ‰")
            query_input = gr.Textbox(label="ê²€ìƒ‰ì–´")
            top_k = gr.Slider(1, 10, 5, step=1, label="ê²°ê³¼ ìˆ˜")
            summary_check = gr.Checkbox(label="ìš”ì•½", value=False)
            recommend_check = gr.Checkbox(label="ì¶”ì²œ", value=True)
            btn = gr.Button("ê²€ìƒ‰")
            results = gr.Markdown()
            recs = gr.Markdown()
            btn.click(self.search_interface, [query_input, top_k, summary_check, recommend_check], [results, recs])
        return demo


def main():
    # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    config_path = project_root / "config" / "config.yaml"

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    logger.info("Starting AI Agent...")
    pipeline = AIAgentPipeline(str(config_path))

    # ì±—ë´‡ UI ìƒì„±
    ui = GradioChatInterface(pipeline)
    demo = ui.create_ui()

    # ì„œë²„ ì‹œì‘
    logger.info("Launching Gradio Chatbot interface...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,  # Docker ì»¨í…Œì´ë„ˆ ì™¸ë¶€ ì ‘ì†ì„ ìœ„í•´ ê³µê°œ URL ìƒì„±
        quiet=False
    )


if __name__ == "__main__":
    main()
